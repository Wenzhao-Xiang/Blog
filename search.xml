<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>CVPR2021 安全AI挑战者计划第六期赛道2：ImageNet无限制对抗攻击 TOP 2 比赛思路</title>
      <link href="/2021/04/25/tianchi2021top2/"/>
      <url>/2021/04/25/tianchi2021top2/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><div align='center' ><font size='6'>CVPR2021 安全AI挑战者计划第六期赛道2：ImageNet无限制对抗攻击 TOP 2 比赛思路</font></div><h2 id="背景介绍">背景介绍</h2><p>深度神经网络在图像识别、自动驾驶和医学图像分析等领域得到广泛的应用。然而，深度神经网络在实际应用中面临诸多问题，最近的研究表明，深度神经网络非常容易受到对抗样本的攻击。现在研究主要关注的是p-范数下的攻击和防御，但是p-范数的扰动需要控制其扰动量来确保人眼无法察觉。而实际场景中，深度模型遇到的更多威胁来自于非限制扰动对抗样本，即攻击者在图像上进行大范围且可见的修改，使得模型误识别的同时不影响人的正常观察。</p><h2 id="赛题分析">赛题分析</h2><p>1.由于比赛是无限制对抗攻击，没有对图像修改程度做出严格限定，但是图像质量和黑盒攻击的成功率之间存在平衡性问题。<br>2.传统的扰动方法在人眼观察下，具有较明显的差别，需要寻找新的思路解决这一问题。<br>3.比赛无法通过query的方式获取到黑盒模型的任何信息，对攻击的有效性提出了更高的挑战。</p><h2 id="思路">思路</h2><p>在图像质量的评估上，人眼对高频信息的变化不敏感，而对低频信息的变化较为敏感，考虑将扰动转换到频域上，提升图像质量。傅里叶变换可以将图像从空间域转换到频域，有助于对信号不同频域信息的解耦，增强对图像质量的控制。</p><h2 id="训练技巧">训练技巧</h2><p>1.在低频信号上设置更严格的修改限制，在高频信号上设置更宽松的限制，采用不同的学习步长进行优化。<br>2.CW损失函数[1]在表现上比cross-entropy损失函数更有优势。<br>3.引入适当的data augmentation的策略，这一策略主要参考DIM[2]在迁移性上所做出的改进。<br>4.没有使用FID[3]和LPIPS[4]指标作为损失函数的一部分，主要考虑到最后的评判标准是人眼，单纯拟合客观分帮助不是很明显，但是本方法的优点在于图像质量可以通过对不同频段信息的修改进行控制，所以可以通过参数调整，获得较高的图像质量。<br>5.为了提升模型在黑盒上的攻击能力，我们使用了ensemble方法将efficientnet[5]、vit[6]和resnest[7]等模型结合起来，在ensemble适量模型之后，黑盒的攻击能力有所提升。</p><h2 id="样本展示">样本展示</h2><table><thead><tr><th>原图</th><th>对抗样本</th></tr></thead><tbody><tr><td><img src="https://cdn.ipfsscan.io/weibo/large/a0732b55ly1gpvz269v29j20dw0dwwfu.jpg" alt="0"></td><td><img src="https://cdn.ipfsscan.io/weibo/large/a0732b55ly1gpvz3ahqb9j20dw0dwabk.jpg" alt="0"></td></tr><tr><td><img src="https://cdn.ipfsscan.io/weibo/large/a0732b55ly1gpvz2g3tg2j20dw0dwq3r.jpg" alt="9"></td><td><img src="https://cdn.ipfsscan.io/weibo/large/a0732b55ly1gpvz3gisx5j20dw0dwab1.jpg" alt="9"></td></tr><tr><td><img src="https://cdn.ipfsscan.io/weibo/large/a0732b55ly1gpvz33g51nj20dw0dw3yu.jpg" alt="19"></td><td><img src="https://cdn.ipfsscan.io/weibo/large/a0732b55ly1gpvz3ln2ysj20dw0dw0t5.jpg" alt="19"></td></tr></tbody></table><h2 id="参考文献">参考文献</h2><p>[1]Carlini N, Wagner D. Towards evaluating the robustness of neural networks[C]//2017 ieee symposium on security and privacy (sp). IEEE, 2017: 39-57.</p><p>[2]Xie C, Zhang Z, Zhou Y, et al. Improving transferability of adversarial examples with input diversity[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2019: 2730-2739.</p><p>[3]Heusel M, Ramsauer H, Unterthiner T, et al. Gans trained by a two time-scale update rule converge to a local nash equilibrium[J]. arXiv preprint arXiv:1706.08500, 2017.</p><p>[4]Zhang R, Isola P, Efros A A, et al. The unreasonable effectiveness of deep features as a perceptual metric[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2018: 586-595.</p><p>[5]Tan M, Le Q. Efficientnet: Rethinking model scaling for convolutional neural networks[C]//International Conference on Machine Learning. PMLR, 2019: 6105-6114.</p><p>[6]Dosovitskiy A, Beyer L, Kolesnikov A, et al. An image is worth 16x16 words: Transformers for image recognition at scale[J]. arXiv preprint arXiv:2010.11929, 2020.</p><p>[7]Zhang H, Wu C, Zhang Z, et al. Resnest: Split-attention networks[J]. arXiv preprint arXiv:2004.08955, 2020.</p>]]></content>
      
      
      <categories>
          
          <category> Adversarial Attack </category>
          
      </categories>
      
      
        <tags>
            
            <tag> tianchi challenge </tag>
            
            <tag> unretricted attack </tag>
            
            <tag> frequency domain </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CVPR2021 安全AI挑战者计划第六期赛道2：ImageNet无限制对抗攻击 TOP 7 比赛思路</title>
      <link href="/2021/04/25/tianchi2021top7/"/>
      <url>/2021/04/25/tianchi2021top7/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><!-- # CVPR2021 安全AI挑战者计划第六期赛道2：ImageNet无限制对抗攻击 TOP 7 比赛思路 --><div align='center' ><font size='6'>CVPR2021 安全AI挑战者计划第六期赛道2：ImageNet无限制对抗攻击 TOP 7 比赛思路</font></div><h2 id="背景介绍">背景介绍</h2><p>深度学习在当下许多视觉任务中都取得了卓越的性能，但是深度神经网络很容易遭受输入上微小和不可察觉的干扰导致的误分类（对抗样本）。而事实上，无限制的对抗样本（对图像进行大范围且可见的修改）往往会对深度学习模型的安全性产生更大的威胁，因为其会使得模型误识别的同时不影响人的正常观察。无限制对抗攻击是近两年来对抗领域的一个热门方向，本次比赛意图在纯黑盒模型的模式下对ImageNet上的无限制对抗攻击场景进行进一步的探索，可以进一步提炼并总结无限制攻击的一些创新而有效的方案，在学术上推动对抗攻击领域的发展。</p><h2 id="赛题分析">赛题分析</h2><ol><li>比赛攻击形式为无限制攻击，因此可以采取生成、变换、Patch攻击、扰动等各种不同的攻击方法。</li><li>比赛引入了对图像质量的评价标准，在初赛和复赛中采取客观评价指标，主要是由 FID（自然真实程度）、LPIPS（和原图的感知距离） 两个指标来衡量，而决赛中采用主观评价指标，依靠专家直接对图像进行打分来评判。因此靠拟合客观分实际也是没用的。</li><li>比赛为纯黑盒攻击，无法拿到模型的结构信息及梯度信息，同时也无法获得模型对某输入的输出，因此只能依靠提高攻击方法的迁移能力来提高攻击成功率。</li></ol><h2 id="解题思路">解题思路</h2><ol><li>通过尝试我们发现，perturbation的攻击方式反而在攻击成功率与图像质量的综合表现上更加，因此我们的主体方案都是围绕perturbation的攻击方式。为了保证迁移能力，我们主体采用的是DI-MI-FGSM[1]（简称DIM），采用动量因子$\mu=1.0$，输入变换概率$p=0.7$，迭代次数为$n=80$，扰动上限为$\epsilon=16/255$。</li><li>通过对loss的尝试，我们发现，CW loss[2]相较于CE loss，能在不明显损失图像质量的情况下较大幅度提高样本的攻击成功率（白盒+黑盒），其他一些loss（如DLR loss[3]）相较于CW loss无明显提升，因此我们最终采用CW loss。值得一提的是，我们没有将客观指标引入loss，因为过拟合客观指标对于决赛是不利的。</li><li>考虑到黑盒模型中可能包含一些防御模型，我们尝试了Translation-Invariant[4]（简称TI）方法，该方法能较大的提高攻击方法在防御模型上的迁移能力。我们将TI方法与DIM方法结合得到TI-DIM，但在实际测试中，其并没有表现出对迁移能力的提高，有可能是黑盒模型中并没有设置防御模型。同时，TI方法对于noise的滤波会导致perturbation更明显，降低图像质量，因此我们最终没有采用。</li><li>对于$L_p$范数的探讨，从经验而言，$L_2$范数下生成的对抗样本图像质量会略优于$L_\infty$范数。在实际尝试时，感觉两者并无太大的差异，最终还是采用$L_\infty$范数。</li><li>图像预处理部分，首先对图像进行resize时，我们发现双三次插值（Bicubic）相比双线性插值（Bilinear）和最邻近插值（Nearest），效果更好。其次，考虑到比赛可能会对noise类型的攻击进行限制，我们在预处理部分引入了gaussian_blur，并令其参与到梯度更新图之中。</li><li>为了进一步提高模型的迁移能力，我们使用了Model Ensemble方法，通过集成不同模型的logits进行攻击。尝试中发现，集成当前SOTA的大模型（efficientnet、vit等），相比集成更多的小模型效果更好。</li></ol><h2 id="结果总结">结果总结</h2><ul><li>代码分享：<a href="https://github.com/Wenzhao-Xiang/transfer_attack_for_tianchi2021">https://github.com/Wenzhao-Xiang/transfer_attack_for_tianchi2021</a></li></ul><h2 id="样本展示">样本展示</h2><table><thead><tr><th>原始图像</th><th>对抗样本</th></tr></thead><tbody><tr><td><img src="https://cdn.ipfsscan.io/weibo/large/a0732b55ly1gpvxylvjwhj20dw0dwwfu.jpg" alt="0"></td><td><img src="https://cdn.ipfsscan.io/weibo/large/a0732b55ly1gpvy02sftwj20dw0dwmyq.jpg" alt="0"></td></tr><tr><td><img src="https://cdn.ipfsscan.io/weibo/large/a0732b55ly1gpvxzg2p5ij20dw0dwwev.jpg" alt="5"></td><td><img src="https://cdn.ipfsscan.io/weibo/large/a0732b55ly1gpvy07ytnhj20dw0dw3zf.jpg" alt="5"></td></tr><tr><td><img src="https://cdn.ipfsscan.io/weibo/large/a0732b55ly1gpvxzrg53fj20dw0dwt9j.jpg" alt="26"></td><td><img src="https://cdn.ipfsscan.io/weibo/large/a0732b55ly1gpvy0e4temj20dw0dwwfo.jpg" alt="26"></td></tr></tbody></table><h2 id="参考文献">参考文献</h2><p>[1] Xie, Cihang, et al. “Improving transferability of adversarial examples with input diversity.” Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2019.</p><p>[2] Carlini N ,  Wagner D . Towards Evaluating the Robustness of Neural Networks[J]. IEEE, 2017.</p><p>[3] Croce F, Hein M. Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks[C]//International Conference on Machine Learning. PMLR, 2020: 2206-2216.</p><p>[4] Dong Y, Pang T, Su H, et al. Evading defenses to transferable adversarial examples by translation-invariant attacks[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2019: 4312-4321.</p>]]></content>
      
      
      <categories>
          
          <category> Adversarial Attack </category>
          
      </categories>
      
      
        <tags>
            
            <tag> tianchi challenge </tag>
            
            <tag> unretricted attack </tag>
            
            <tag> perturbation-based attack </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CIKM2020 安全AI挑战者计划第四期：通用目标检测的对抗攻击 TOP 8 比赛思路-Adversarial Contour</title>
      <link href="/2020/10/22/Contour/"/>
      <url>/2020/10/22/Contour/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><div align='center' ><font size='6'>CIKM2020 安全AI挑战者计划第四期：通用目标检测的对抗攻击 TOP 8 比赛思路-Adversarial Contour</font></div><h1>环境配置</h1><p>系统：Ubuntu18.04<br>硬件：GTX 1080TI * 8<br>软件版本：</p><p align="center"><strong>表1</strong>：软件版本<div class="center"><table><thead><tr><th>软件</th><th>版本号</th><th>软件</th><th>版本号</th></tr></thead><tbody><tr><td>python</td><td>3.7.8</td><td>cython</td><td>0.29.21</td></tr><tr><td>pytorch</td><td>1.6.0</td><td>matplotlib</td><td>3.3.1</td></tr><tr><td>scikit-image</td><td>0.17.2</td><td>mmcv-full</td><td>1.0.5</td></tr><tr><td>scipy</td><td>1.5.2</td><td>mmdet*</td><td>2.3.0rc0+head</td></tr><tr><td>torchvision</td><td>0.7.0</td><td>numpy</td><td>1.19.1</td></tr><tr><td>tqdm</td><td>4.48.2</td><td>opencv-python</td><td>4.4.0.42</td></tr><tr><td>cudatoolkit</td><td>10.2.89</td><td>pillow</td><td>6.2.2</td></tr></tbody></table></div></p><p>* 实验中所使用的mmdetection是我们在该版本号下结合本项目需求修改得到的版本。</p><h1>解题思路</h1><p>根据题目的设置，需要在贴图攻击的连通域数量、贴图面积受限的情况下完成对目标检测器的黑盒、白盒攻击。同时，在500*500像素的图像上，总的贴图面积不超过5000pixel，总的连通域个数不超过10个。</p><p>我们将问题分为两个部分，像素修改方法和攻击区域选择方法。</p><p>首先介绍比较简单的像素修改（纹理优化）的方法。我们使用了比赛方提供的两个白盒模型利用迭代优化的方法对贴图区域内的像素进行优化。总的优化方法有两种：</p><ol><li><p>使用画面中detector检测到的最大置信度作为损失函数，通过梯度反向传递，对patch内像素进行优化。在YOLO中使用最大object confidence，而在faster-rcnn中，使用最大class confidence，因为faster-rcnn的输出中不使用object confidence的信息。<br>$$ p = arg\min(\max(conf(x+p))) $$</p></li><li><p>使用画面中detector检测到的较高置信度的加和作为损失函数。这种方法相比上一种，训练过程更加稳定，不易出现剧烈抖动，并且能够兼顾高低置信度的bounding box。<br>$$ p = arg\min(\sum(topk(conf(x+p)))) $$</p></li></ol><p>最后我们选择的损失函数是Yolo的最大置信度和faster-rcnn的较高置信度之和来共同进行优化。</p><p>现在介绍比较复杂的攻击区域的选择方法。我们尝试了多种方法进行攻击区域的选择。</p><h2 id="在所有检测到的bbox中心设置固定的patch">在所有检测到的bbox中心设置固定的patch</h2><p>该方法比较简单，只对patch的纹理进行优化，而没有对patch的形状及位置进行任何优化，因此攻击的能力很低，仅作为我们的baseline方案，就不展示分数了。攻击的示意图如图1所示。</p><p align="center"><img src="https://cdn.ipfsscan.io/weibo/large/002WaWKVly1guji7rax0dj60dw0dw46r02.jpg" alt="outline1" width="500" data-width="500" data-height="500"> <br> <strong>图1</strong>：基于bbox中心位置的攻击结果图</p><h2 id="在图像左上角设置固定的patch">在图像左上角设置固定的patch</h2><p>基于D-patch论文，我们尝试了攻击区域固定在左上角的攻击方式。为了匹配D-patch的设计方式，我们将损失函数设置为target攻击的损失函数，且target类别为background，希望使得检测器将目光聚焦在背景上而忽略所有的前景物体。但是经过实验发现，D-patch攻击方法并不奏效，可能是我们的代码设置有问题。最后这一方法没有进行线上测试。</p><h2 id="基于目标轮廓的攻击方法">基于目标轮廓的攻击方法</h2><p>在前搜索区域的方法中，我们发现我们的攻击方法有时会倾向于选择前景物体与背景相接的区块，这启发了我们对于边缘区域重要性的思考。同时，我们发现patch能够影响的区域越大，攻击也就越有效。之前的思路是patch都是一块一块的，攻击像素的聚集性比较强，那么能不能把patch做的分散一点从而实现更大的影响区域覆盖呢？我们基于这一想法，设计了边缘攻击方法。</p><p>算法1：边缘轮廓攻击区域</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">利用mmdetection预训练的Mask-RCNN对1000张样本进行实例分割，得到一系列物体所对应的mask矩阵及对应的置信度。</span><br><span class="line">对某一样本，将其全部预测进行nms处理，将面积重复超过0.5的mask进行筛选，避免出现过多的重复攻击区域。</span><br><span class="line">将每一物体所对应的mask进行一轮kernel大小3<span class="emphasis">*3的腐蚀，并与原来的mask做差得到边缘轮廓。将这些轮廓依次添加到画布上，直到其总像素数超过5000，得到初步的攻击区域。</span></span><br><span class="line"><span class="emphasis">对攻击区域进行联通域检测，将其中面积过小的联通域删除，得到最终的边缘轮廓攻击区域。</span></span><br></pre></td></tr></table></figure><p>我们首先使用mmdetection中的MaskRCNN(mask_rcnn_r50_fpn_1x_coco_20200205-d4b0c5d6.pth)来获取图像中物体的mask，在边缘上我们通过利用opencv的erode函数，用一个3*3的kernel进行一次迭代，获取了一个足够细的并且联通的边缘，这便是基于边缘攻击的攻击区域。这里我们为了避免出现多个物体重叠过多，导致边缘距离十分接近的情况，对物体的mask区域进行了类似nms的操作，去除了一部分重叠率较高的轮廓线，尽可能减少了位置相近的攻击区域。之后，将每一个物体的预测所得到的轮廓加入patch图中，得到全图的攻击区域。基于这些区域，我们进一步进行纹理优化，最终得分为2898分。这一基于边缘攻击的结果如示意图2，3所示。</p><p align="center"><img src="https://cdn.ipfsscan.io/weibo/large/002WaWKVly1guji7rjt57j60dw0dw46h02.jpg" alt="outline1" width="500" data-width="500" data-height="500"> <br> <strong>图2</strong>：边缘轮廓攻击示意图2</p><p align="center"><img src="https://cdn.ipfsscan.io/weibo/large/002WaWKVly1guji7rfmzdj60dw0dwtlh02.jpg" alt="outline2" width="500" data-width="500" data-height="500"> <br> <strong>图3</strong>：边缘轮廓攻击示意图2</p><p>为了增加该方法在黑盒上的表现，我们额外选取了一些模型，与YOLO以及Faster RCNN进行联合训练，包括Mask RCNN、Cascade R-CNN、Hybrid Task Cascade、RetinaNet等等。但Ensemble的方法并不能提高黑盒攻击的性能。经过验证我们发现，基于边缘攻击的方法对于白盒模型的过拟合现象比较严重，迁移性很差。例如，基于Faster RCNN Res50上训练得到有效攻击图像，对于的Faster RCNN Res101的攻击有效率就已经非常低了。为了提高攻击算法对黑盒模型的迁移性，从而提高在黑盒模型上的表现，我们尝试了在梯度参与patch更新之前，对梯度先进行卷积操作，卷积核为预先设定好的高斯核。该方法最早是由2019年CVPR Oral paper“Evading Defenses to Transferable Adversarial Examples by Translation-Invariant Attacks”所提出，该方法被证实能提高所生成的对抗样本对黑盒模型的迁移性。在实际测试中我们发现，该方法确实能有效抑制patch攻击在白盒模型上的过拟合现象，提高其在黑盒模型上的鲁棒性，但是也会一定程度影响对抗样本在白盒模型上的表现，需要权衡选择。</p><p>除了提高算法的迁移性这一思路外，我们也尝试了进一步减小轮廓的攻击范围。我们将攻击区域从中间分割，抹去约20%的攻击像素，将闭合的轮廓一分为二，并进一步进行训练。这样的攻击区域，经过纹理优化之后，对一些易于攻击的样本取得了在攻击面积分数上的进一步提高，达到了2992分。这一定程度上也说明在攻击样本时，物体不同位置的轮廓对于物体的检测贡献会有一些差异。这一方法的截取方式示意图如图4所示。</p><p align="center"><img src="https://cdn.ipfsscan.io/weibo/large/002WaWKVly1guji7rowtsj60dw0dw45f02.jpg" alt="outline2" width="500" data-width="500" data-height="500"> <br> <strong>图3</strong>：截掉中间约20%轮廓的攻击示意图</p><p>最终，通过一些微小的优化和多次训练结果的择优选取，我们的提交结果取得了最后的3006分。这一结果中，绝大部分的样本是利用边缘攻击以及分割后边缘攻击的方法训练得到的。</p><p>从最终提交版本所对应的方案来看，在获取攻击区域上，不是我们最开始想要使用的迭代的搜索，而是使用了已有模型，根据预测结果进行处理，直接获取。而这种攻击物体轮廓区域的思路在此之前也比较少见，是一种很有趣的思路，在此之后我们也会继续研究轮廓对物体检测模型的影响。与此同时，由于mask rcnn本身就是用于物体检测的一个很有力的模型，通过它的预测来获取不同样本的攻击区域，从效果上应该是具有普遍性的，这一方法的有效性，并不局限于本次比赛的数据集。此外，我们的算法是确定性的，可以通过MaskRCNN模型的预测，轮廓的处理和梯度下降的优化得到近似目前的结果分数，从期望的角度讲，本提交结果是完全可以复现的。</p>]]></content>
      
      
      <categories>
          
          <category> Adversarial Attack </category>
          
      </categories>
      
      
        <tags>
            
            <tag> object detection </tag>
            
            <tag> adversarial attack </tag>
            
            <tag> tianchi challenge </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>GSoC2019 - Improve the performance of JavaScript version of OpenCV (OpenCV.js)</title>
      <link href="/2020/02/01/gsoc2019/"/>
      <url>/2020/02/01/gsoc2019/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><!-- # [GSoC 2019] Improve the performance of JavaScript version of OpenCV (OpenCV.js) --><!-- #### Wenzhao-Xiang | [github](https://github.com/Wenzhao-Xiang) | [twitter](https://twitter.com/Wenzhao_Xiang) | [email](mailto:winzard35@gmail.com) --><h2 id="Overview">Overview</h2><ul><li><a href="https://summerofcode.withgoogle.com/projects/#5715812734730240">Project Link</a></li><li>Proposal:  <a href="https://docs.google.com/document/d/1Nx3MDFnM47kumdyyUzju2xn3loiZnf0HsmhmCzAJfKo/edit?usp=sharing">Improve the performance of JavaScript version of OpenCV (OpenCV.js)</a></li><li>Mentor: Ningxin Hu, Vitaly Tuzov</li><li>Organization: OpenCV</li></ul><h2 id="Introduction">Introduction</h2><p><a href="https://docs.opencv.org/3.4/df/d0a/tutorial_js_intro.html">OpenCV.js</a> is a JavaScript binding for selected subset of OpenCV functions for the web platform. It allows emerging web applications with multimedia processing to benefit from the wide variety of vision functions available in OpenCV. OpenCV.js leverages Emscripten to compile OpenCV functions into asm.js or WebAssembly targets, and provides a JavaScript APIs for web application to access them.</p><p>However, now the performance of OpenCV.js still have a big gap with Native, and it can’t support real-time tasks very well, such as face detection and face recognition. The biggest reason is that the current version of OpenCV.js runs with single thread and no SIMD, which greatly wastes the parallel computing power of the CPU.</p><p>But at this time, WebAssembly can reduce the performance gap between Web and Native. WebAssembly now support multi-threading with Web Worker and shareArrayBuffer, and is going on supporting new v128 value types used for SIMD, which can both improve the parallel computing capability on Web.</p><p>Therefore, the main goal of this project is to speedup OpenCV.js by multi-threading and SIMD.</p><h2 id="Work-structure">Work structure</h2><h3 id="Create-the-base-of-OpenCV-js-performance-test">Create the base of OpenCV.js performance test</h3><p><a href="https://benchmarkjs.com">Benchmark.js</a> is a benchmarking library that supports high-resolution timers &amp; returns statistically significant results. And the OpenCV.js performance test tool is based on it. Now we add three kernels of imgproc module into these performance test, which are <code>cvtColor</code>, <code>Resize</code> and <code>Threshold</code>. And all the performance tests are based on native performance test.</p><p>To run performance tests, launch a local web server in &lt;build_dir&gt;/bin folder. For example, node http-server which serves on <code>localhost:8080</code>. If you want to test <code>threshold</code>, please navigate the web browser to <code>http://localhost:8080/perf/perf_imgproc/perf_threshold.html</code>. You need to input the test parameter like <code>(1920x1080, CV_8UC1, THRESH_BINARY)</code>, and then click the <code>Run</code> button to run the case. And if you don’t input the parameter, it will run all the cases of this kernel.</p><p>You can also run tests using Node.js. For example, run <code>threshold</code> with parameter <code>(1920x1080, CV_8UC1, THRESH_BINARY)</code>:</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> bin/perf</span><br><span class="line">npm install</span><br><span class="line">node perf_threshold.js --test_param_filter=<span class="string">&quot;(1920x1080, CV_8UC1, THRESH_BINARY)&quot;</span></span><br></pre></td></tr></table></figure><h3 id="Optimize-the-OpenCV-js-performance-by-WebAssembly-threads">Optimize the OpenCV.js performance by WebAssembly threads</h3><p>WebAssembly now support multi-threading with Web Worker and SharedArrayBuffer (i.e., <a href="https://github.com/WebAssembly/threads">WebAssembly threads</a>). Developers are able to use Emscripten to translate the pthreads based native code to Web Workers and SharedArrayBuffer based WebAssembly code. So we leverage this capability to translate <a href="https://docs.opencv.org/3.4.1/d7/dff/tutorial_how_to_use_OpenCV_parallel_for_.html">OpenCV pthreads API</a> implementation into equivalent WebAssembly code by using Web Workers with SharedArrayBuffer. The multithreading version of OpenCV.js will have a pool of Web Workers and will schedule a worker when a new thread is being spawn. And this optimization can only be used in browser as node.js have no Web Worker API.</p><p>We expose two new API <code>cv.parallel_pthreads_set_threads_num(number)</code> and <code>cv.parallel_pthreads_get_threads_num()</code>, so we can use the former to set threads number dynamically and use the latter to get the current threads number. And the default threads number is the logic core number of the device.</p><h3 id="Optimize-the-OpenCV-js-performance-by-WebAssembly-SIMD">Optimize the OpenCV.js performance by WebAssembly SIMD</h3><p>WebAssembly is adding the support of SIMD128 instructions (i.e., <a href="https://github.com/WebAssembly/simd/blob/master/proposals/simd/SIMD.md">WebAssembly SIMD</a>). This features has been landed in V8/Chromium behind a developer flag. On the tooling side, the WebAssembly SIMD builtins has been added to LLVM compiler and Emscripten has released the first version of WebAssembly intrinsics. So we can use Emscripten LLVM upstream backend to translate the native vectorization implementation to WebAssembly SIMD128 instructions and deploy them to browsers.</p><p>Today’s OpenCV Universal intrinsics implementation have multiple backends for different architectures, such as SSE, NEON, AXV and VSX. Therefore, we added a new WebAssembly SIMD backend by using LLVM WebAssembly builtins and WebAssembly intrinsics.</p><p>We also enabled the WebAssembly intrinsics tests by compiling the native intrinsics tests to WebAssembly. With this tool, we can easily test whether our WebAssembly backend implementation of Universal Intrinsics is right. And now it pass all the tests.</p><p>The SIMD optimization is experimental as WebAssembly SIMD is still in development. Therefore, the simd version of OpenCV.js built by latest LLVM upstream may not work with the stable browser or old version of Node.js. Please use the latest version of unstable browser or Node.js to get new features, like <code>Chrome Dev</code>.</p><h2 id="Result">Result</h2><p>For OpenCV kernels, take <code>Threshold</code> kernel with parameter <code>(1920x1080, CV_8UC1, THRESH_BINARY)</code> as example:</p><p>OS: Ubuntu 16.04.5<br><br>Emscripten: 1.38.42, LLVM upstream backend<br><br>Browser: Chrome, Version 78.0.3880.4 (Official Build) dev (64-bit)<br><br>Hardware: Core™ i7-8700 CPU @ 3.20GHz with 12 logical cores:</p><table><thead><tr><th>OpenCV.js Build</th><th>Mean Time (ms)</th><th>Speedup (to scalar)</th></tr></thead><tbody><tr><td>scalar</td><td>1.164</td><td>1</td></tr><tr><td>threads</td><td>0.261</td><td>4.45</td></tr><tr><td>simd</td><td>0.123</td><td>9.46</td></tr><tr><td>threads + simd</td><td>0.039</td><td>29.84</td></tr></tbody></table><p>For real case, take OpenCV.js face recognition sample as example:</p><p>OS: Ubuntu Linux 16.04.5<br><br>Emscripten: 1.38.42, LLVM upstream backend<br><br>Browser: Chrome, Version 78.0.3880.4 (Official Build) dev (64-bit)<br><br>Hardware: Intel® Core™ i7-8700 CPU @ 3.20GHz with 12 logical cores</p><table><thead><tr><th>OpenCV.js Build</th><th>FPS</th><th>Speedup (to scalar)</th></tr></thead><tbody><tr><td>scalar</td><td>3</td><td>1</td></tr><tr><td>threads</td><td>10</td><td>3.33</td></tr><tr><td>simd</td><td>12</td><td>4</td></tr><tr><td>threads + simd</td><td>26</td><td>8.6</td></tr></tbody></table><h2 id="Future-Work">Future Work</h2><ol><li><p>Add more modules and kernels into performance test, like <code>core</code>, <code>feature2d</code>, <code>video</code> and so on.</p></li><li><p>Optimize the Universal Intrinsics WebAssembly backend with the development of WebAssembly SIMD.</p></li></ol><h2 id="OpenCV-js-Demos">OpenCV.js Demos</h2><p><a href="https://wenzhao-xiang.github.io/opencvjs/index.html">OpenCV.js Demos</a> (May need the latest version of Chrome-Dev)<br><br><a href="https://youtu.be/ertdEzqE6bI">My video report for GSoC on Youtube</a></p><h2 id="Commits-List">Commits List</h2><p><a href="https://github.com/opencv/opencv/pull/15371">The PR</a><br><br><a href="https://github.com/Wenzhao-Xiang/opencv/commit/82e98faa65ba070a83d6d040be778f2b1fab6e29">The list of my commits</a></p>]]></content>
      
      
      <categories>
          
          <category> Web </category>
          
      </categories>
      
      
        <tags>
            
            <tag> opencvjs </tag>
            
            <tag> webassembly </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
